{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Anti-Echo Chamber - News Scraper & Processor\n",
        "\n",
        "This notebook scrapes news articles from RSS feeds, processes them through the anti-echo chamber pipeline, and uploads only metadata + embeddings to Hugging Face.\n",
        "\n",
        "**Key Features:**\n",
        "- Scrapes from diverse news sources\n",
        "- Processes articles through topic modeling and stance classification\n",
        "- Stores only embeddings and metadata (no full text)\n",
        "- Uploads processed data to Hugging Face for sharing\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AHMerrill/anti-echo-2/blob/main/notebooks/scraper_artifacts.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q chromadb sentence-transformers transformers huggingface-hub pymupdf beautifulsoup4 scikit-learn nltk pyyaml feedparser trafilatura\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "import torch\n",
        "os.environ[\"CHROMA_TELEMETRY_ENABLED\"] = \"false\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "# GPU/CPU Detection and Configuration\n",
        "def setup_device():\n",
        "    \"\"\"Detect and configure device for optimal performance.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "        print(f\"üöÄ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "        print(\"üíª Using CPU (GPU not available)\")\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "    \n",
        "    print(f\"‚úÖ Device configured: {device}\")\n",
        "    return device\n",
        "\n",
        "# Setup device\n",
        "device = setup_device()\n",
        "\n",
        "# Manual device override (uncomment if needed)\n",
        "# device = \"cpu\"  # Force CPU usage\n",
        "# device = \"cuda\"  # Force GPU usage (if available)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Core Library and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the core library and configs from GitHub\n",
        "!git clone https://github.com/AHMerrill/anti-echo-2.git temp_repo\n",
        "!cp -r temp_repo/* ./\n",
        "!rm -rf temp_repo\n",
        "\n",
        "# Verify files are downloaded\n",
        "!ls -la\n",
        "\n",
        "# Optional: Set up Git for pushing results back (if you want to save results to GitHub)\n",
        "# Uncomment the lines below and add your PAT if you want to push results back\n",
        "# !git config --global user.email \"your-email@example.com\"\n",
        "# !git config --global user.name \"Your Name\"\n",
        "# !git remote set-url origin https://YOUR_PAT@github.com/AHMerrill/anti-echo-2.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from anti_echo_core import AntiEchoCore\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize the core system with detected device\n",
        "print(\"Initializing Anti-Echo Chamber system...\")\n",
        "core = AntiEchoCore(\"config/config.yaml\", device=device)\n",
        "print(f\"‚úì System initialized successfully on {core.device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hugging Face Authentication\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hugging Face Authentication\n",
        "from huggingface_hub import HfApi, create_repo, login\n",
        "import getpass\n",
        "\n",
        "def get_hf_token():\n",
        "    \"\"\"Get Hugging Face token from user input or local file.\"\"\"\n",
        "    # Try to load from local file first (for local development)\n",
        "    try:\n",
        "        with open(\"hf_token.txt\", \"r\") as f:\n",
        "            token = f.read().strip()\n",
        "        print(\"‚úÖ Hugging Face token loaded from local file\")\n",
        "        return token\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "    \n",
        "    # If not found locally, ask user to input it\n",
        "    print(\"üîë Hugging Face Authentication Required\")\n",
        "    print(\"   Get your token from: https://huggingface.co/settings/tokens\")\n",
        "    print(\"   (Make sure it has 'Write' permissions)\")\n",
        "    \n",
        "    # Use getpass for secure input (hides the token as you type)\n",
        "    token = getpass.getpass(\"Enter your Hugging Face token: \").strip()\n",
        "    \n",
        "    if token:\n",
        "        print(\"‚úÖ Hugging Face token received\")\n",
        "        return token\n",
        "    else:\n",
        "        print(\"‚ùå No token provided\")\n",
        "        return None\n",
        "\n",
        "# Get the token\n",
        "hf_token = get_hf_token()\n",
        "\n",
        "if not hf_token:\n",
        "    print(\"‚ö†Ô∏è No HF token available. Upload functionality will be disabled.\")\n",
        "    print(\"   You can still run the scraping and processing locally.\")\n",
        "\n",
        "print(\"Hugging Face authentication ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration - Easy Customization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION - EASILY CUSTOMIZE SCRAPING PARAMETERS\n",
        "# =============================================================================\n",
        "\n",
        "# Number of articles to scrape per RSS feed (adjust as needed)\n",
        "MAX_ARTICLES_PER_FEED = 5  # Change this number to scrape more/fewer articles\n",
        "\n",
        "print(f\"üìä Configuration:\")\n",
        "print(f\"   Max articles per feed: {MAX_ARTICLES_PER_FEED}\")\n",
        "print(f\"   (Adjust MAX_ARTICLES_PER_FEED above to change this)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RSS Feed sources for diverse political perspectives\n",
        "RSS_FEEDS = {\n",
        "    # Conservative sources\n",
        "    \"fox_news\": \"https://feeds.foxnews.com/foxnews/politics\",\n",
        "    \"daily_caller\": \"https://dailycaller.com/feed/\",\n",
        "    \"federalist\": \"https://thefederalist.com/feed/\",\n",
        "    \"reason\": \"https://reason.com/feed/\",\n",
        "    \n",
        "    # Liberal sources\n",
        "    \"npr\": \"https://feeds.npr.org/1001/rss.xml\",\n",
        "    \"vox\": \"https://www.vox.com/rss/index.xml\",\n",
        "    \"msnbc\": \"https://www.msnbc.com/feeds/latest\",\n",
        "    \"propublica\": \"https://www.propublica.org/feeds/propublica/main\",\n",
        "    \n",
        "    # International sources\n",
        "    \"bbc\": \"http://feeds.bbci.co.uk/news/rss.xml\",\n",
        "    \"guardian\": \"https://www.theguardian.com/world/rss\",\n",
        "    \"al_jazeera\": \"https://www.aljazeera.com/xml/rss/all.xml\",\n",
        "    \"france24\": \"https://www.france24.com/en/rss\",\n",
        "    \n",
        "    # Academic/Think tank\n",
        "    \"conversation\": \"https://theconversation.com/global/rss\",\n",
        "    \"city_journal\": \"https://www.city-journal.org/feed\",\n",
        "    \"dw\": \"https://rss.dw.com/rdf/rss-en-all\"\n",
        "}\n",
        "\n",
        "print(f\"Configured {len(RSS_FEEDS)} RSS feeds for scraping\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import feedparser\n",
        "import trafilatura\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse\n",
        "import hashlib\n",
        "\n",
        "def scrape_and_process_articles(max_articles_per_feed=None, dataset_name=\"anti-echo-chamber-data\"):\n",
        "    \"\"\"Complete pipeline: scrape, process, and upload to Hugging Face.\"\"\"\n",
        "    \n",
        "    # Use global config if not specified\n",
        "    if max_articles_per_feed is None:\n",
        "        max_articles_per_feed = MAX_ARTICLES_PER_FEED\n",
        "    \n",
        "    print(\"üöÄ Starting Anti-Echo Chamber Pipeline\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"üñ•Ô∏è Running on: {core.device}\")\n",
        "    print(f\"üìä Max articles per feed: {max_articles_per_feed}\")\n",
        "    if core.device == \"cuda\":\n",
        "        print(f\"‚ö° GPU acceleration enabled - expect faster processing!\")\n",
        "    else:\n",
        "        print(f\"üêå CPU mode - processing will be slower but still functional\")\n",
        "    \n",
        "    # Check for existing articles in HF dataset to avoid duplicates\n",
        "    existing_article_ids = set()\n",
        "    try:\n",
        "        from datasets import load_dataset\n",
        "        dataset = load_dataset(dataset_name, split=\"train\")\n",
        "        existing_article_ids = set(dataset[\"id\"])\n",
        "        print(f\"üìã Found {len(existing_article_ids)} existing articles in dataset\")\n",
        "    except:\n",
        "        print(\"üìã No existing dataset found, starting fresh\")\n",
        "    \n",
        "    # Step 1: Scrape articles\n",
        "    print(\"\\nüì∞ Step 1: Scraping news articles...\")\n",
        "    all_articles = []\n",
        "    \n",
        "    for source_name, feed_url in RSS_FEEDS.items():\n",
        "        print(f\"Scraping {source_name}...\")\n",
        "        try:\n",
        "            feed = feedparser.parse(feed_url)\n",
        "            articles = []\n",
        "            \n",
        "            for entry in feed.entries[:max_articles_per_feed]:\n",
        "                try:\n",
        "                    article_text = trafilatura.extract(entry.link)\n",
        "                    if article_text and len(article_text) > 200:\n",
        "                        article = {\n",
        "                            \"title\": entry.get(\"title\", \"\"),\n",
        "                            \"url\": entry.link,\n",
        "                            \"source\": source_name,\n",
        "                            \"published\": entry.get(\"published\", \"\"),\n",
        "                            \"text\": article_text,\n",
        "                            \"id\": hashlib.md5(article_text.encode()).hexdigest()\n",
        "                        }\n",
        "                        articles.append(article)\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "            \n",
        "            print(f\"  ‚úì Scraped {len(articles)} articles from {source_name}\")\n",
        "            all_articles.extend(articles)\n",
        "            time.sleep(1)  # Be respectful to servers\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ‚úó Error scraping {source_name}: {e}\")\n",
        "    \n",
        "    # Remove duplicates (both URL and existing article IDs)\n",
        "    seen_urls = set()\n",
        "    unique_articles = []\n",
        "    for article in all_articles:\n",
        "        article_id = article[\"id\"]\n",
        "        if (article[\"url\"] not in seen_urls and \n",
        "            article_id not in existing_article_ids):\n",
        "            seen_urls.add(article[\"url\"])\n",
        "            unique_articles.append(article)\n",
        "        elif article_id in existing_article_ids:\n",
        "            print(f\"  ‚è≠Ô∏è Skipping existing article: {article['title'][:50]}...\")\n",
        "    \n",
        "    print(f\"\\n‚úì Total new articles to process: {len(unique_articles)}\")\n",
        "    print(f\"‚è≠Ô∏è Skipped {len(all_articles) - len(unique_articles)} duplicate/existing articles\")\n",
        "    \n",
        "    print(f\"\\n‚úì Total unique articles scraped: {len(unique_articles)}\")\n",
        "    \n",
        "    if not unique_articles:\n",
        "        print(\"‚ùå No articles were scraped\")\n",
        "        return\n",
        "    \n",
        "    # Step 2: Process articles\n",
        "    print(\"\\nüîç Step 2: Processing articles through pipeline...\")\n",
        "    processed_articles = []\n",
        "    \n",
        "    for i, article in enumerate(unique_articles):\n",
        "        try:\n",
        "            print(f\"  Processing {i+1}/{len(unique_articles)}: {article['title'][:50]}...\")\n",
        "            \n",
        "            processed = core.process_article(article)\n",
        "            if processed:\n",
        "                core.upsert_to_chroma(processed)\n",
        "                metadata_only = core.export_metadata_only(processed)\n",
        "                processed_articles.append(metadata_only)\n",
        "                print(f\"    ‚úì Processed: {processed['political_leaning']} / {processed['implied_stance']}\")\n",
        "            else:\n",
        "                print(f\"    ‚úó Failed to process\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"    ‚úó Error: {e}\")\n",
        "            continue\n",
        "    \n",
        "    print(f\"\\n‚úì Successfully processed {len(processed_articles)} articles\")\n",
        "    \n",
        "    if not processed_articles:\n",
        "        print(\"‚ùå No articles were successfully processed\")\n",
        "        return\n",
        "    \n",
        "    # Step 3: Upload to Hugging Face (if token available)\n",
        "    if hf_token:\n",
        "        print(\"\\n‚òÅÔ∏è Step 3: Uploading to Hugging Face...\")\n",
        "        \n",
        "        try:\n",
        "            # Create dataset repository\n",
        "            create_repo(dataset_name, repo_type=\"dataset\", exist_ok=True, token=hf_token)\n",
        "            print(f\"‚úì Created/verified dataset: {dataset_name}\")\n",
        "            \n",
        "            # Convert to DataFrame and upload\n",
        "            df = pd.DataFrame(processed_articles)\n",
        "            df.to_csv(\"processed_articles.csv\", index=False)\n",
        "            \n",
        "            api = HfApi(token=hf_token)\n",
        "            api.upload_file(\n",
        "                path_or_fileobj=\"processed_articles.csv\",\n",
        "                path_in_repo=\"processed_articles.csv\",\n",
        "                repo_id=dataset_name,\n",
        "                repo_type=\"dataset\"\n",
        "            )\n",
        "            \n",
        "            print(f\"‚úì Uploaded {len(processed_articles)} articles to Hugging Face\")\n",
        "            print(f\"  Dataset: https://huggingface.co/datasets/{dataset_name}\")\n",
        "            \n",
        "            # Also save embeddings separately\n",
        "            embeddings_data = {\n",
        "                \"topic_embeddings\": [article[\"topic_vectors\"] for article in processed_articles],\n",
        "                \"stance_embeddings\": [article[\"stance_embedding\"] for article in processed_articles],\n",
        "                \"metadata\": [{k: v for k, v in article.items() if k not in [\"topic_vectors\", \"stance_embedding\"]} for article in processed_articles]\n",
        "            }\n",
        "            \n",
        "            with open(\"embeddings_data.json\", \"w\") as f:\n",
        "                json.dump(embeddings_data, f, indent=2)\n",
        "            \n",
        "            api.upload_file(\n",
        "                path_or_fileobj=\"embeddings_data.json\",\n",
        "                path_in_repo=\"embeddings_data.json\",\n",
        "                repo_id=dataset_name,\n",
        "                repo_type=\"dataset\"\n",
        "            )\n",
        "            \n",
        "            print(f\"‚úì Uploaded embeddings data to Hugging Face\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Upload error: {e}\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è Step 3: Skipping Hugging Face upload (no token provided)\")\n",
        "        print(\"   Articles processed and stored locally in ChromaDB\")\n",
        "        print(\"   To upload later, run this cell again with a valid HF token\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Pipeline completed successfully!\")\n",
        "    print(f\"   - Scraped: {len(unique_articles)} articles\")\n",
        "    print(f\"   - Processed: {len(processed_articles)} articles\")\n",
        "    print(f\"   - Uploaded: Metadata + embeddings only (no full text)\")\n",
        "    \n",
        "    return processed_articles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure dataset name\n",
        "dataset_name = \"zanimal/anti-echo-chamber-data\"  # Default dataset\n",
        "# Uncomment the line below to input a different dataset name\n",
        "# dataset_name = input(\"Enter dataset name (or press Enter for default): \").strip() or \"zanimal/anti-echo-chamber-data\"\n",
        "\n",
        "print(f\"üîß Debug Info:\")\n",
        "print(f\"   Dataset: {dataset_name}\")\n",
        "print(f\"   HF Token: {'‚úÖ Available' if hf_token else '‚ùå Not provided'}\")\n",
        "print(f\"   Device: {core.device}\")\n",
        "print(f\"   RSS Feeds: {len(RSS_FEEDS)} configured\")\n",
        "\n",
        "# Run the complete pipeline\n",
        "# This will scrape articles, process them, and upload to Hugging Face\n",
        "try:\n",
        "    processed_articles = scrape_and_process_articles(\n",
        "        max_articles_per_feed=MAX_ARTICLES_PER_FEED, \n",
        "        dataset_name=dataset_name\n",
        "    )\n",
        "    print(f\"\\nüéâ Pipeline completed! Processed {len(processed_articles) if processed_articles else 0} articles\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Pipeline failed with error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verification - Check Processing Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify that articles were processed correctly\n",
        "if 'processed_articles' in locals() and processed_articles:\n",
        "    print(\"üîç VERIFICATION RESULTS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Check processing statistics\n",
        "    total_processed = len(processed_articles)\n",
        "    print(f\"‚úÖ Total articles processed: {total_processed}\")\n",
        "    \n",
        "    # Check political leanings distribution\n",
        "    leanings = [article.get('political_leaning', 'unknown') for article in processed_articles]\n",
        "    leaning_counts = {}\n",
        "    for leaning in leanings:\n",
        "        leaning_counts[leaning] = leaning_counts.get(leaning, 0) + 1\n",
        "    \n",
        "    print(f\"\\nüìä Political Leanings Distribution:\")\n",
        "    for leaning, count in sorted(leaning_counts.items()):\n",
        "        percentage = (count / total_processed) * 100\n",
        "        print(f\"   {leaning}: {count} ({percentage:.1f}%)\")\n",
        "    \n",
        "    # Check implied stances distribution\n",
        "    stances = [article.get('implied_stance', 'unknown') for article in processed_articles]\n",
        "    stance_counts = {}\n",
        "    for stance in stances:\n",
        "        stance_counts[stance] = stance_counts.get(stance, 0) + 1\n",
        "    \n",
        "    print(f\"\\nüìä Implied Stances Distribution:\")\n",
        "    for stance, count in sorted(stance_counts.items()):\n",
        "        percentage = (count / total_processed) * 100\n",
        "        print(f\"   {stance}: {count} ({percentage:.1f}%)\")\n",
        "    \n",
        "    # Check topics distribution\n",
        "    all_topics = []\n",
        "    for article in processed_articles:\n",
        "        all_topics.extend(article.get('topics', []))\n",
        "    \n",
        "    topic_counts = {}\n",
        "    for topic in all_topics:\n",
        "        topic_counts[topic] = topic_counts.get(topic, 0) + 1\n",
        "    \n",
        "    print(f\"\\nüìä Top Topics Detected:\")\n",
        "    for topic, count in sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "        print(f\"   {topic}: {count}\")\n",
        "    \n",
        "    # Check vectorization\n",
        "    articles_with_vectors = sum(1 for article in processed_articles \n",
        "                              if article.get('topic_vectors') and article.get('stance_embedding'))\n",
        "    print(f\"\\nüî¢ Vectorization Status:\")\n",
        "    print(f\"   Articles with topic vectors: {articles_with_vectors}/{total_processed}\")\n",
        "    print(f\"   Articles with stance embeddings: {articles_with_vectors}/{total_processed}\")\n",
        "    \n",
        "    # Check ChromaDB storage\n",
        "    try:\n",
        "        topic_count = core.topic_coll.count()\n",
        "        stance_count = core.stance_coll.count()\n",
        "        print(f\"\\nüíæ ChromaDB Storage:\")\n",
        "        print(f\"   Topic collection: {topic_count} vectors\")\n",
        "        print(f\"   Stance collection: {stance_count} vectors\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ö†Ô∏è ChromaDB check failed: {e}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Verification complete! All systems working properly.\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No processed articles found. Run the pipeline first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect a sample processed article to verify quality\n",
        "if 'processed_articles' in locals() and processed_articles:\n",
        "    print(\"üîç SAMPLE ARTICLE INSPECTION\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Show first processed article\n",
        "    sample = processed_articles[0]\n",
        "    print(f\"üì∞ Sample Article:\")\n",
        "    print(f\"   Title: {sample.get('title', 'N/A')[:80]}...\")\n",
        "    print(f\"   Source: {sample.get('source', 'N/A')}\")\n",
        "    print(f\"   Political Leaning: {sample.get('political_leaning', 'N/A')}\")\n",
        "    print(f\"   Implied Stance: {sample.get('implied_stance', 'N/A')}\")\n",
        "    print(f\"   Topics: {', '.join(sample.get('topics', []))}\")\n",
        "    print(f\"   Summary: {sample.get('summary', 'N/A')[:100]}...\")\n",
        "    print(f\"   Text Length: {sample.get('text_length', 0)} characters\")\n",
        "    \n",
        "    # Check vector dimensions\n",
        "    topic_vectors = sample.get('topic_vectors', [])\n",
        "    stance_embedding = sample.get('stance_embedding', [])\n",
        "    \n",
        "    print(f\"\\nüî¢ Vector Analysis:\")\n",
        "    print(f\"   Topic vectors: {len(topic_vectors)} clusters\")\n",
        "    if topic_vectors:\n",
        "        print(f\"   Topic vector dimension: {len(topic_vectors[0]) if topic_vectors[0] else 'N/A'}\")\n",
        "    print(f\"   Stance embedding dimension: {len(stance_embedding) if stance_embedding else 'N/A'}\")\n",
        "    \n",
        "    # Show a few more samples\n",
        "    print(f\"\\nüìã Quick Sample of Other Articles:\")\n",
        "    for i, article in enumerate(processed_articles[1:4], 1):\n",
        "        print(f\"   {i}. {article.get('title', 'N/A')[:50]}... | {article.get('political_leaning', 'N/A')} | {article.get('implied_stance', 'N/A')}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Sample inspection complete!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No processed articles found. Run the pipeline first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
