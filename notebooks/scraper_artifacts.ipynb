{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Anti-Echo Chamber - News Scraper & Processor\n",
        "\n",
        "This notebook scrapes news articles from RSS feeds, processes them through the anti-echo chamber pipeline, and uploads only metadata + embeddings to Hugging Face.\n",
        "\n",
        "**Key Features:**\n",
        "- Scrapes from diverse news sources\n",
        "- Processes articles through topic modeling and stance classification\n",
        "- Stores only embeddings and metadata (no full text)\n",
        "- Uploads processed data to Hugging Face for sharing\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AHMerrill/anti-echo-2/blob/main/notebooks/scraper_artifacts.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q chromadb sentence-transformers transformers huggingface-hub pymupdf beautifulsoup4 scikit-learn nltk pyyaml feedparser trafilatura\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"CHROMA_TELEMETRY_ENABLED\"] = \"false\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Core Library and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the core library and configs from GitHub\n",
        "!git clone https://github.com/AHMerrill/anti-echo-2.git temp_repo\n",
        "!cp -r temp_repo/* ./\n",
        "!rm -rf temp_repo\n",
        "\n",
        "# Verify files are downloaded\n",
        "!ls -la\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from anti_echo_core import AntiEchoCore\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize the core system\n",
        "print(\"Initializing Anti-Echo Chamber system...\")\n",
        "core = AntiEchoCore(\"config/config.yaml\")\n",
        "print(\"‚úì System initialized successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hugging Face Authentication\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hugging Face Authentication\n",
        "# You'll need to get a token from: https://huggingface.co/settings/tokens\n",
        "# Then run: huggingface-cli login\n",
        "\n",
        "from huggingface_hub import HfApi, create_repo, login\n",
        "\n",
        "# Login to Hugging Face (run this cell and follow the prompts)\n",
        "# login()\n",
        "\n",
        "# Or set your token directly (not recommended for security)\n",
        "# os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"your_token_here\"\n",
        "\n",
        "print(\"Hugging Face authentication ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RSS Feed sources for diverse political perspectives\n",
        "RSS_FEEDS = {\n",
        "    # Conservative sources\n",
        "    \"fox_news\": \"https://feeds.foxnews.com/foxnews/politics\",\n",
        "    \"daily_caller\": \"https://dailycaller.com/feed/\",\n",
        "    \"federalist\": \"https://thefederalist.com/feed/\",\n",
        "    \"reason\": \"https://reason.com/feed/\",\n",
        "    \n",
        "    # Liberal sources\n",
        "    \"npr\": \"https://feeds.npr.org/1001/rss.xml\",\n",
        "    \"vox\": \"https://www.vox.com/rss/index.xml\",\n",
        "    \"msnbc\": \"https://www.msnbc.com/feeds/latest\",\n",
        "    \"propublica\": \"https://www.propublica.org/feeds/propublica/main\",\n",
        "    \n",
        "    # International sources\n",
        "    \"bbc\": \"http://feeds.bbci.co.uk/news/rss.xml\",\n",
        "    \"guardian\": \"https://www.theguardian.com/world/rss\",\n",
        "    \"al_jazeera\": \"https://www.aljazeera.com/xml/rss/all.xml\",\n",
        "    \"france24\": \"https://www.france24.com/en/rss\",\n",
        "    \n",
        "    # Academic/Think tank\n",
        "    \"conversation\": \"https://theconversation.com/global/rss\",\n",
        "    \"city_journal\": \"https://www.city-journal.org/feed\",\n",
        "    \"dw\": \"https://rss.dw.com/rdf/rss-en-all\"\n",
        "}\n",
        "\n",
        "print(f\"Configured {len(RSS_FEEDS)} RSS feeds for scraping\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import feedparser\n",
        "import trafilatura\n",
        "import pandas as pd\n",
        "from urllib.parse import urlparse\n",
        "import hashlib\n",
        "\n",
        "def scrape_and_process_articles(max_articles_per_feed=5, dataset_name=\"anti-echo-chamber-data\"):\n",
        "    \"\"\"Complete pipeline: scrape, process, and upload to Hugging Face.\"\"\"\n",
        "    \n",
        "    print(\"üöÄ Starting Anti-Echo Chamber Pipeline\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Step 1: Scrape articles\n",
        "    print(\"\\nüì∞ Step 1: Scraping news articles...\")\n",
        "    all_articles = []\n",
        "    \n",
        "    for source_name, feed_url in RSS_FEEDS.items():\n",
        "        print(f\"Scraping {source_name}...\")\n",
        "        try:\n",
        "            feed = feedparser.parse(feed_url)\n",
        "            articles = []\n",
        "            \n",
        "            for entry in feed.entries[:max_articles_per_feed]:\n",
        "                try:\n",
        "                    article_text = trafilatura.extract(entry.link)\n",
        "                    if article_text and len(article_text) > 200:\n",
        "                        article = {\n",
        "                            \"title\": entry.get(\"title\", \"\"),\n",
        "                            \"url\": entry.link,\n",
        "                            \"source\": source_name,\n",
        "                            \"published\": entry.get(\"published\", \"\"),\n",
        "                            \"text\": article_text,\n",
        "                            \"id\": hashlib.md5(article_text.encode()).hexdigest()\n",
        "                        }\n",
        "                        articles.append(article)\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "            \n",
        "            print(f\"  ‚úì Scraped {len(articles)} articles from {source_name}\")\n",
        "            all_articles.extend(articles)\n",
        "            time.sleep(1)  # Be respectful to servers\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ‚úó Error scraping {source_name}: {e}\")\n",
        "    \n",
        "    # Remove duplicates\n",
        "    seen_urls = set()\n",
        "    unique_articles = []\n",
        "    for article in all_articles:\n",
        "        if article[\"url\"] not in seen_urls:\n",
        "            seen_urls.add(article[\"url\"])\n",
        "            unique_articles.append(article)\n",
        "    \n",
        "    print(f\"\\n‚úì Total unique articles scraped: {len(unique_articles)}\")\n",
        "    \n",
        "    if not unique_articles:\n",
        "        print(\"‚ùå No articles were scraped\")\n",
        "        return\n",
        "    \n",
        "    # Step 2: Process articles\n",
        "    print(\"\\nüîç Step 2: Processing articles through pipeline...\")\n",
        "    processed_articles = []\n",
        "    \n",
        "    for i, article in enumerate(unique_articles):\n",
        "        try:\n",
        "            print(f\"  Processing {i+1}/{len(unique_articles)}: {article['title'][:50]}...\")\n",
        "            \n",
        "            processed = core.process_article(article)\n",
        "            if processed:\n",
        "                core.upsert_to_chroma(processed)\n",
        "                metadata_only = core.export_metadata_only(processed)\n",
        "                processed_articles.append(metadata_only)\n",
        "                print(f\"    ‚úì Processed: {processed['political_leaning']} / {processed['implied_stance']}\")\n",
        "            else:\n",
        "                print(f\"    ‚úó Failed to process\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"    ‚úó Error: {e}\")\n",
        "            continue\n",
        "    \n",
        "    print(f\"\\n‚úì Successfully processed {len(processed_articles)} articles\")\n",
        "    \n",
        "    if not processed_articles:\n",
        "        print(\"‚ùå No articles were successfully processed\")\n",
        "        return\n",
        "    \n",
        "    # Step 3: Upload to Hugging Face\n",
        "    print(\"\\n‚òÅÔ∏è Step 3: Uploading to Hugging Face...\")\n",
        "    \n",
        "    try:\n",
        "        # Create dataset repository\n",
        "        create_repo(dataset_name, repo_type=\"dataset\", exist_ok=True)\n",
        "        print(f\"‚úì Created/verified dataset: {dataset_name}\")\n",
        "        \n",
        "        # Convert to DataFrame and upload\n",
        "        df = pd.DataFrame(processed_articles)\n",
        "        df.to_csv(\"processed_articles.csv\", index=False)\n",
        "        \n",
        "        api = HfApi()\n",
        "        api.upload_file(\n",
        "            path_or_fileobj=\"processed_articles.csv\",\n",
        "            path_in_repo=\"processed_articles.csv\",\n",
        "            repo_id=dataset_name,\n",
        "            repo_type=\"dataset\"\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úì Uploaded {len(processed_articles)} articles to Hugging Face\")\n",
        "        print(f\"  Dataset: https://huggingface.co/datasets/{dataset_name}\")\n",
        "        \n",
        "        # Also save embeddings separately\n",
        "        embeddings_data = {\n",
        "            \"topic_embeddings\": [article[\"topic_vectors\"] for article in processed_articles],\n",
        "            \"stance_embeddings\": [article[\"stance_embedding\"] for article in processed_articles],\n",
        "            \"metadata\": [{k: v for k, v in article.items() if k not in [\"topic_vectors\", \"stance_embedding\"]} for article in processed_articles]\n",
        "        }\n",
        "        \n",
        "        with open(\"embeddings_data.json\", \"w\") as f:\n",
        "            json.dump(embeddings_data, f, indent=2)\n",
        "        \n",
        "        api.upload_file(\n",
        "            path_or_fileobj=\"embeddings_data.json\",\n",
        "            path_in_repo=\"embeddings_data.json\",\n",
        "            repo_id=dataset_name,\n",
        "            repo_type=\"dataset\"\n",
        "        )\n",
        "        \n",
        "        print(f\"‚úì Uploaded embeddings data to Hugging Face\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Upload error: {e}\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Pipeline completed successfully!\")\n",
        "    print(f\"   - Scraped: {len(unique_articles)} articles\")\n",
        "    print(f\"   - Processed: {len(processed_articles)} articles\")\n",
        "    print(f\"   - Uploaded: Metadata + embeddings only (no full text)\")\n",
        "    \n",
        "    return processed_articles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the complete pipeline\n",
        "# This will scrape articles, process them, and upload to Hugging Face\n",
        "processed_articles = scrape_and_process_articles(max_articles_per_feed=3, dataset_name=\"anti-echo-chamber-data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
